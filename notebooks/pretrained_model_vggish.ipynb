{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up paths and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvggish import vggish\n",
    "from torchvggish import vggish_input\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "if not os.path.exists(\"./notebooks\"):\n",
    "    %cd ..\n",
    "\n",
    "from src.training import do_train, do_test\n",
    "from src.audio_dataset_processor import DAPSDatasetProcessor\n",
    "from src.dataset import BalancedBatchSampler\n",
    "from src.config import VALID_ACCESS_LABELS, DATA_DIR\n",
    "\n",
    "wandb_enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, lr=0.001, epochs=40, batch_size=32):\n",
    "        self.learning_rate = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally initialize W&B project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choose device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define VGGish model and feature_extraction code\n",
    "`VGGish` takes `.wav` files and have its own method to set proper settings on sampled audio. Inference gives 128 floats (these should be semantic features of clip) for every second of recording, so we early return if clip duration is not divisible by `SPLIT_SECONDS`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vggish()\n",
    "model.eval()\n",
    "\n",
    "SPLIT_SECONDS = 3\n",
    "\n",
    "def preprocess_audio(file_path, target_sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Load a .wav file, convert to mono, and preprocess into log-Mel spectrogram.\n",
    "    \"\"\"\n",
    "    audio, sr = librosa.load(file_path, sr=target_sample_rate, mono=True)\n",
    "    \n",
    "    if len(audio) < target_sample_rate:\n",
    "        padding = target_sample_rate - len(audio)\n",
    "        audio = np.pad(audio, (0, padding), mode='constant')\n",
    "\n",
    "    mel_spec = vggish_input.waveform_to_examples(audio, sr)\n",
    "    return torch.tensor(mel_spec)\n",
    "\n",
    "def extract_features(file_paths):\n",
    "    features = []\n",
    "    for file in file_paths:\n",
    "        mel_spec = preprocess_audio(file)\n",
    "        speaker_id = os.path.basename(file).split(\"_\")[0]\n",
    "        label = int(speaker_id in VALID_ACCESS_LABELS)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            file_features = model(mel_spec)\n",
    "        \n",
    "        for idx, feature in enumerate(file_features):\n",
    "            if idx >= len(file_features) - (len(file_features) % SPLIT_SECONDS):\n",
    "                break\n",
    "            features.append((torch.tensor(feature), label))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use common code to split `.wav` files into datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_directories=['ipadflat_confroom1', 'ipadflat_office1', 'ipad_balcony1', 'ipad_bedroom1', 'ipad_confroom1', 'ipad_confroom2', 'ipad_livingroom1', 'ipad_office1', 'ipad_office2', 'iphone_balcony1', 'iphone_bedroom1', 'iphone_livingroom1']\n",
    "dataset_processor = DAPSDatasetProcessor(DATA_DIR, VALID_ACCESS_LABELS, allowed_directories)\n",
    "dataset_processor.compute_statistics()\n",
    "train_set, validate_set, test_set = dataset_processor.get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define `VGGish` specific Dataset class\n",
    "It takes batches of `SPLIT_SECONDS` 1-second features embedded using `VGGish`. This class would be used as dataset for simple classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class VGGishDataset(Dataset):\n",
    "    def __init__(self, files):\n",
    "        self.data = extract_features(files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.data) / SPLIT_SECONDS)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        spectrogram, label = self.data[idx * SPLIT_SECONDS]\n",
    "        spectrogram2, label = self.data[idx * SPLIT_SECONDS + 1]\n",
    "        spectrogram3, label = self.data[idx * SPLIT_SECONDS + 2]\n",
    "        return torch.cat((spectrogram, spectrogram2, spectrogram3), dim=0), label\n",
    "    \n",
    "train_dataset = VGGishDataset(train_set)\n",
    "val_dataset = VGGishDataset(validate_set)\n",
    "test_dataset = VGGishDataset(test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Define simple classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 2\n",
    "INPUT_DIM = 128 * SPLIT_SECONDS\n",
    "HIDDEN_DIM = 256\n",
    "N_HIDDEN_LAYERS = 1\n",
    "\n",
    "class ClassifierForVGGish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassifierForVGGish, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(INPUT_DIM, HIDDEN_DIM))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        for _ in range(N_HIDDEN_LAYERS - 1):\n",
    "            layers.append(nn.Linear(HIDDEN_DIM, HIDDEN_DIM))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        layers.append(nn.Linear(HIDDEN_DIM, N_CLASSES))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Define config and run train, validation loops and test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassifierForVGGish()\n",
    "config = Config(batch_size=32, epochs=40, lr=0.001)\n",
    "name = \"VGGish_transfer_learning\"\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "sampler = BalancedBatchSampler(train_dataset, config.batch_size)\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "run = do_train(name, train_loader, val_loader, config, model, criterion, optimizer, device, wandb_enabled)\n",
    "do_test(name, test_loader, model.__class__, run, device, wandb_enabled)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
