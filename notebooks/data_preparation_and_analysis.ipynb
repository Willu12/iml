{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, run this cell to set up paths and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "if not os.path.exists(r\"./notebooks\"):\n",
    "    %cd ..\n",
    "\n",
    "from src.audio_processor import AudioProcessor, compute_duration_statistics\n",
    "from src.audio_dataset_processor import AudioDatasetProcessor\n",
    "from src.data_processing import SOAAudioClips, save_mean_std, compute_mean_std_from_images\n",
    "from src.dataset_analysis import duration_statistics\n",
    "from src.config import VALID_ACCESS_LABELS, TRAIN_DIR, TEST_DIR, VAL_DIR, DATA_DIR, DATASET_DIR\n",
    "\n",
    "os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "os.makedirs(TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(VAL_DIR, exist_ok=True)\n",
    "os.makedirs(TEST_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Split all allowed .wav files\n",
    "We are using [DAPS](https://zenodo.org/records/4660670) dataset. It has several directories available in which there are .wav files of 5 scripts read by 20 speakers. Directories differ from each other with augmentation, which is labeled by `room` and `recording device`. In this cell we are specifying allowed directories, their contents are being discovered and splitted into 3 datasets (training, validation and test). The same speaker with the same script with different augmentation cannot be in the same dataset - `AudioDatasetProcessor` class take care of that. `balance` parameter set to true balances authorized and unauthorized speakers file count in training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_directories=['ipadflat_confroom1', 'ipadflat_office1', 'ipad_balcony1', 'ipad_bedroom1', 'ipad_confroom1', 'ipad_confroom2', 'ipad_livingroom1', 'ipad_office1', 'ipad_office2', 'iphone_balcony1', 'iphone_bedroom1', 'iphone_livingroom1']\n",
    "dataset_processor = AudioDatasetProcessor(DATA_DIR, VALID_ACCESS_LABELS, allowed_directories)\n",
    "dataset_processor.compute_statistics()\n",
    "train_set, validate_set, test_set = dataset_processor.get_datasets(balanced=True) # if you want unbalanced set parameter to False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Duration statistics of .wav files\n",
    "We first are checking full clips duration statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soa_train_full_clips = SOAAudioClips(train_set)\n",
    "soa_test_full_clips = SOAAudioClips(validate_set)\n",
    "soa_val_full_clips = SOAAudioClips(test_set)\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(\"Training set:\")\n",
    "print(duration_statistics(soa_train_full_clips.clips))\n",
    "\n",
    "print(\"Validation set:\")\n",
    "print(duration_statistics(soa_test_full_clips.clips))\n",
    "\n",
    "print(\"Test set:\")\n",
    "print(duration_statistics(soa_val_full_clips.clips))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split into few seconds clips\n",
    "Now we decide to split full clips into few seconds subclips, they are filtered so only clips with more than `0.5` of recording has speech detected by `webrtcvad`. We are doing this to establish the same size of input for CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP_DURATION_SECONDS = 3\n",
    "\n",
    "def process_audio_clips(audio_processor, soa_full_clips, output_dir, clip_duration=CLIP_DURATION_SECONDS):\n",
    "    \"\"\"Processes and splits audio clips, saves spectrograms, and computes statistics.\"\"\"\n",
    "    all_clips = []\n",
    "    for file_path, full_clip in tqdm(soa_full_clips, desc=\"Processing audio clips\"):\n",
    "        clips = audio_processor.split_audio(full_clip, clip_duration)\n",
    "        speech_clips = audio_processor.filter_speech_clips(clips)\n",
    "        all_clips.extend(speech_clips)\n",
    "\n",
    "        for i, clip in enumerate(speech_clips):\n",
    "            spectrogram = audio_processor.create_spectrogram(clip)\n",
    "            output_path = os.path.join(\n",
    "                output_dir, f\"{os.path.basename(file_path).split('.')[0]}_{i}_clip.png\"\n",
    "            )\n",
    "            audio_processor.save_spectrogram(spectrogram, output_path)\n",
    "\n",
    "    print(compute_duration_statistics(all_clips))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then after processing we save log mel grayscale spectrograms into separate directories for every dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_processor = AudioProcessor()\n",
    "\n",
    "print(\"Preprocessed Train Dataset:\")\n",
    "process_audio_clips(audio_processor, soa_train_full_clips, TRAIN_DIR)\n",
    "\n",
    "print(\"\\nPreprocessed Validation Dataset:\")\n",
    "process_audio_clips(audio_processor, soa_val_full_clips, VAL_DIR)\n",
    "\n",
    "print(\"\\nPreprocessed Test Dataset:\")\n",
    "process_audio_clips(audio_processor, soa_test_full_clips, TEST_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mean and standard deviation of training dataset\n",
    "It is important to normalize data for our Neural Networks. It creates better distribution. It should smoothen loss function plane (so it would be easier to find global minimum). We use only training dataset for this purpose and save it to JSON file next to datasets' image directories.\n",
    "\n",
    "Effect of normalization for example CNN can be seen [here](https://wandb.ai/mytkom-warsaw-university-of-technology/iml/reports/Effect-of-normalization-input-for-TutorialCNN--VmlldzoxMDUxMTI1OQ?accessToken=s67utpfjryb4um1240bd56i51zo5oy2bj0gbaqqz79z3hnnabkub1rdhsamhwd2v)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = compute_mean_std_from_images(TRAIN_DIR)\n",
    "print(f\"Mean: {mean}, Standard deviation: {std}\")\n",
    "save_mean_std(mean, std, f\"{DATASET_DIR}/scaling_params.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
