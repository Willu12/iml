{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## First, run this cell to set up paths and import dependencies"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "\n",
            "from tqdm import tqdm\n",
            "\n",
            "if not os.path.exists(r\"./notebooks\"):\n",
            "    %cd ..\n",
            "\n",
            "from src.audio_processor import AudioProcessor\n",
            "from src.audio_dataset_processor import DAPSDatasetProcessor\n",
            "from src.data_processing import SOAAudioClips, save_mean_std, compute_mean_std_from_images\n",
            "from src.dataset_analysis import duration_statistics\n",
            "from src.config import VALID_ACCESS_LABELS, TRAIN_DIR, TEST_DIR, VAL_DIR, DATA_DIR, DATASET_DIR\n",
            "\n",
            "os.makedirs(DATASET_DIR, exist_ok=True)\n",
            "os.makedirs(TRAIN_DIR, exist_ok=True)\n",
            "os.makedirs(VAL_DIR, exist_ok=True)\n",
            "os.makedirs(TEST_DIR, exist_ok=True)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 1. Split all allowed .wav files\n",
            "We are using [DAPS](https://zenodo.org/records/4660670) dataset. It has several directories available in which there are .wav files of 5 scripts read by 20 speakers. Directories differ from each other with augmentation, which is labeled by `room` and `recording device`. In this cell we are specifying allowed directories, their contents are being discovered and splitted into 3 datasets (training, validation and test). The same script cannot be in the same dataset - `AudioDatasetProcessor` class take care of that. Balancing classes is done using batch_sampler in DataLoader by undersampling major class."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Searching in allowed directories: ['ipadflat_confroom1', 'ipadflat_office1', 'ipad_balcony1', 'ipad_bedroom1', 'ipad_confroom1']\n",
                  "Found 500 .wav files in directory './data'\n",
                  "--- Train Set Statistics ---\n",
                  "Total Samples: 300\n",
                  "Total Speakers: 20\n",
                  "Authorized Samples: 90\n",
                  "Unauthorized Samples: 210\n",
                  "Authorized to Unauthorized Ratio: 90:210\n",
                  "\n",
                  "Samples per Speaker:\n",
                  "  f1: 15\n",
                  "  f10: 15\n",
                  "  f2: 15\n",
                  "  f3: 15\n",
                  "  f4: 15\n",
                  "  f5: 15\n",
                  "  f6: 15\n",
                  "  f7: 15\n",
                  "  f8: 15\n",
                  "  f9: 15\n",
                  "  m1: 15\n",
                  "  m10: 15\n",
                  "  m2: 15\n",
                  "  m3: 15\n",
                  "  m4: 15\n",
                  "  m5: 15\n",
                  "  m6: 15\n",
                  "  m7: 15\n",
                  "  m8: 15\n",
                  "  m9: 15\n",
                  "\n",
                  "--- Validate Set Statistics ---\n",
                  "Total Samples: 100\n",
                  "Total Speakers: 20\n",
                  "Authorized Samples: 30\n",
                  "Unauthorized Samples: 70\n",
                  "Authorized to Unauthorized Ratio: 30:70\n",
                  "\n",
                  "Samples per Speaker:\n",
                  "  f1: 5\n",
                  "  f10: 5\n",
                  "  f2: 5\n",
                  "  f3: 5\n",
                  "  f4: 5\n",
                  "  f5: 5\n",
                  "  f6: 5\n",
                  "  f7: 5\n",
                  "  f8: 5\n",
                  "  f9: 5\n",
                  "  m1: 5\n",
                  "  m10: 5\n",
                  "  m2: 5\n",
                  "  m3: 5\n",
                  "  m4: 5\n",
                  "  m5: 5\n",
                  "  m6: 5\n",
                  "  m7: 5\n",
                  "  m8: 5\n",
                  "  m9: 5\n",
                  "\n",
                  "--- Test Set Statistics ---\n",
                  "Total Samples: 100\n",
                  "Total Speakers: 20\n",
                  "Authorized Samples: 30\n",
                  "Unauthorized Samples: 70\n",
                  "Authorized to Unauthorized Ratio: 30:70\n",
                  "\n",
                  "Samples per Speaker:\n",
                  "  f1: 5\n",
                  "  f10: 5\n",
                  "  f2: 5\n",
                  "  f3: 5\n",
                  "  f4: 5\n",
                  "  f5: 5\n",
                  "  f6: 5\n",
                  "  f7: 5\n",
                  "  f8: 5\n",
                  "  f9: 5\n",
                  "  m1: 5\n",
                  "  m10: 5\n",
                  "  m2: 5\n",
                  "  m3: 5\n",
                  "  m4: 5\n",
                  "  m5: 5\n",
                  "  m6: 5\n",
                  "  m7: 5\n",
                  "  m8: 5\n",
                  "  m9: 5\n",
                  "\n"
               ]
            }
         ],
         "source": [
            "allowed_directories=['ipadflat_confroom1', 'ipadflat_office1', 'ipad_balcony1', 'ipad_bedroom1', 'ipad_confroom1', 'ipad_confroom2', 'ipad_livingroom1', 'ipad_office1', 'ipad_office2', 'iphone_balcony1', 'iphone_bedroom1', 'iphone_livingroom1']\n",
            "dataset_processor = DAPSDatasetProcessor(DATA_DIR, VALID_ACCESS_LABELS, allowed_directories)\n",
            "\n",
            "dataset_processor.compute_statistics()\n",
            "train_set, validate_set, test_set = dataset_processor.get_datasets()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 2. Duration statistics of .wav files\n",
            "We first are checking full clips duration statistics."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "\n",
                  "Dataset Statistics:\n",
                  "Training set:\n",
                  "Statistics:\n",
                  "        Total files: 300,\n",
                  "        Total duration: 51263.47 sec,\n",
                  "        Average duration: 170.88 sec, \n",
                  "        Duration range: 136.42 - 224.20 sec\n",
                  "        \n",
                  "Validation set:\n",
                  "Statistics:\n",
                  "        Total files: 100,\n",
                  "        Total duration: 13811.49 sec,\n",
                  "        Average duration: 138.11 sec, \n",
                  "        Duration range: 116.16 - 174.32 sec\n",
                  "        \n",
                  "Test set:\n",
                  "Statistics:\n",
                  "        Total files: 100,\n",
                  "        Total duration: 15406.48 sec,\n",
                  "        Average duration: 154.06 sec, \n",
                  "        Duration range: 131.11 - 194.24 sec\n",
                  "        \n"
               ]
            }
         ],
         "source": [
            "soa_train_full_clips = SOAAudioClips(train_set)\n",
            "soa_test_full_clips = SOAAudioClips(validate_set)\n",
            "soa_val_full_clips = SOAAudioClips(test_set)\n",
            "\n",
            "print(\"\\nDataset Statistics:\")\n",
            "print(\"Training set:\")\n",
            "print(duration_statistics(soa_train_full_clips.clips))\n",
            "\n",
            "print(\"Validation set:\")\n",
            "print(duration_statistics(soa_test_full_clips.clips))\n",
            "\n",
            "print(\"Test set:\")\n",
            "print(duration_statistics(soa_val_full_clips.clips))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 3. Split into few seconds clips\n",
            "Now we decide to split full clips into few seconds subclips, they are filtered so only clips with more than `0.5` of recording has speech detected by `webrtcvad`. We are doing this to establish the same size of input for CNN."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Then after processing we save log mel grayscale spectrograms into separate directories for every dataset."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Preprocessed Train Dataset:\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Processing audio clips: 100%|██████████| 300/300 [01:46<00:00,  2.83it/s]\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Statistics:\n",
                  "        Total files: 18014,\n",
                  "        Total duration: 54042.00 sec,\n",
                  "        Average duration: 3.00 sec, \n",
                  "        Duration range: 3.00 - 3.00 sec\n",
                  "        \n",
                  "\n",
                  "Preprocessed Validation Dataset:\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Processing audio clips: 100%|██████████| 100/100 [00:31<00:00,  3.20it/s]\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Statistics:\n",
                  "        Total files: 5372,\n",
                  "        Total duration: 16116.00 sec,\n",
                  "        Average duration: 3.00 sec, \n",
                  "        Duration range: 3.00 - 3.00 sec\n",
                  "        \n",
                  "\n",
                  "Preprocessed Test Dataset:\n"
               ]
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Processing audio clips:  39%|███▉      | 39/100 [00:11<00:17,  3.43it/s]\n"
               ]
            },
            {
               "ename": "KeyboardInterrupt",
               "evalue": "",
               "output_type": "error",
               "traceback": [
                  "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                  "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
                  "File \u001b[0;32m~/Studia/ml/iml/.venv/lib/python3.9/site-packages/PIL/ImageFile.py:554\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 554\u001b[0m     fh \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mfileno()\n\u001b[1;32m    555\u001b[0m     fp\u001b[39m.\u001b[39mflush()\n",
                  "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
                  "\nDuring handling of the above exception, another exception occurred:\n",
                  "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                  "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(duration_stats)\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mPreprocessed Test Dataset:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m duration_stats \u001b[39m=\u001b[39m audio_processor\u001b[39m.\u001b[39;49mprocess_audio_clips(soa_test_full_clips, TEST_DIR)\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(duration_stats)\n",
                  "File \u001b[0;32m~/Studia/ml/iml/src/audio_processor.py:30\u001b[0m, in \u001b[0;36mAudioProcessor.process_audio_clips\u001b[0;34m(self, soa_full_clips, output_dir, clip_duration)\u001b[0m\n\u001b[1;32m     26\u001b[0m         spectrogram \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_spectrogram(clip)\n\u001b[1;32m     27\u001b[0m         output_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m     28\u001b[0m             output_dir, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(file_path)\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m_clip.png\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m         )\n\u001b[0;32m---> 30\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_spectrogram(spectrogram, output_path)\n\u001b[1;32m     32\u001b[0m \u001b[39mreturn\u001b[39;00m duration_statistics(all_clips)\n",
                  "File \u001b[0;32m~/Studia/ml/iml/src/audio_processor.py:80\u001b[0m, in \u001b[0;36mAudioProcessor.save_spectrogram\u001b[0;34m(self, spectrogram, output_path)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_spectrogram\u001b[39m(\u001b[39mself\u001b[39m, spectrogram, output_path):\n\u001b[1;32m     79\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Saves the spectrogram as an image.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     plt\u001b[39m.\u001b[39;49mimsave(output_path, spectrogram, cmap\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgray\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
                  "File \u001b[0;32m~/Studia/ml/iml/.venv/lib/python3.9/site-packages/matplotlib/pyplot.py:2604\u001b[0m, in \u001b[0;36mimsave\u001b[0;34m(fname, arr, **kwargs)\u001b[0m\n\u001b[1;32m   2600\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(matplotlib\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mimsave)\n\u001b[1;32m   2601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimsave\u001b[39m(\n\u001b[1;32m   2602\u001b[0m     fname: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m os\u001b[39m.\u001b[39mPathLike \u001b[39m|\u001b[39m BinaryIO, arr: ArrayLike, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m   2603\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2604\u001b[0m     matplotlib\u001b[39m.\u001b[39;49mimage\u001b[39m.\u001b[39;49mimsave(fname, arr, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
                  "File \u001b[0;32m~/Studia/ml/iml/.venv/lib/python3.9/site-packages/matplotlib/image.py:1676\u001b[0m, in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1674\u001b[0m pil_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mformat\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mformat\u001b[39m)\n\u001b[1;32m   1675\u001b[0m pil_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mdpi\u001b[39m\u001b[39m\"\u001b[39m, (dpi, dpi))\n\u001b[0;32m-> 1676\u001b[0m image\u001b[39m.\u001b[39;49msave(fname, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpil_kwargs)\n",
                  "File \u001b[0;32m~/Studia/ml/iml/.venv/lib/python3.9/site-packages/PIL/Image.py:2605\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2602\u001b[0m     fp \u001b[39m=\u001b[39m cast(IO[\u001b[39mbytes\u001b[39m], fp)\n\u001b[1;32m   2604\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2605\u001b[0m     save_handler(\u001b[39mself\u001b[39;49m, fp, filename)\n\u001b[1;32m   2606\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   2607\u001b[0m     \u001b[39mif\u001b[39;00m open_fp:\n",
                  "File \u001b[0;32m~/Studia/ml/iml/.venv/lib/python3.9/site-packages/PIL/PngImagePlugin.py:1488\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     single_im \u001b[39m=\u001b[39m _write_multiple_frames(\n\u001b[1;32m   1485\u001b[0m         im, fp, chunk, mode, rawmode, default_image, append_images\n\u001b[1;32m   1486\u001b[0m     )\n\u001b[1;32m   1487\u001b[0m \u001b[39mif\u001b[39;00m single_im:\n\u001b[0;32m-> 1488\u001b[0m     ImageFile\u001b[39m.\u001b[39;49m_save(\n\u001b[1;32m   1489\u001b[0m         single_im,\n\u001b[1;32m   1490\u001b[0m         cast(IO[\u001b[39mbytes\u001b[39;49m], _idat(fp, chunk)),\n\u001b[1;32m   1491\u001b[0m         [ImageFile\u001b[39m.\u001b[39;49m_Tile(\u001b[39m\"\u001b[39;49m\u001b[39mzip\u001b[39;49m\u001b[39m\"\u001b[39;49m, (\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m) \u001b[39m+\u001b[39;49m single_im\u001b[39m.\u001b[39;49msize, \u001b[39m0\u001b[39;49m, rawmode)],\n\u001b[1;32m   1492\u001b[0m     )\n\u001b[1;32m   1494\u001b[0m \u001b[39mif\u001b[39;00m info:\n\u001b[1;32m   1495\u001b[0m     \u001b[39mfor\u001b[39;00m info_chunk \u001b[39min\u001b[39;00m info\u001b[39m.\u001b[39mchunks:\n",
                  "File \u001b[0;32m~/Studia/ml/iml/.venv/lib/python3.9/site-packages/PIL/ImageFile.py:558\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    556\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[1;32m    557\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mAttributeError\u001b[39;00m, io\u001b[39m.\u001b[39mUnsupportedOperation) \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m--> 558\u001b[0m     _encode_tile(im, fp, tile, bufsize, \u001b[39mNone\u001b[39;49;00m, exc)\n\u001b[1;32m    559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(fp, \u001b[39m\"\u001b[39m\u001b[39mflush\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    560\u001b[0m     fp\u001b[39m.\u001b[39mflush()\n",
                  "File \u001b[0;32m~/Studia/ml/iml/.venv/lib/python3.9/site-packages/PIL/ImageFile.py:584\u001b[0m, in \u001b[0;36m_encode_tile\u001b[0;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[39mif\u001b[39;00m exc:\n\u001b[1;32m    582\u001b[0m     \u001b[39m# compress to Python file-compatible object\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m         errcode, data \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mencode(bufsize)[\u001b[39m1\u001b[39m:]\n\u001b[1;32m    585\u001b[0m         fp\u001b[39m.\u001b[39mwrite(data)\n\u001b[1;32m    586\u001b[0m         \u001b[39mif\u001b[39;00m errcode:\n",
                  "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
               ]
            }
         ],
         "source": [
            "audio_processor = AudioProcessor()\n",
            "\n",
            "print(\"Preprocessed Train Dataset:\")\n",
            "duration_stats = audio_processor.process_audio_clips(soa_train_full_clips, TRAIN_DIR)\n",
            "print(duration_stats)\n",
            "\n",
            "print(\"\\nPreprocessed Validation Dataset:\")\n",
            "duration_stats = audio_processor.process_audio_clips(soa_val_full_clips, VAL_DIR)\n",
            "print(duration_stats)\n",
            "\n",
            "print(\"\\nPreprocessed Test Dataset:\")\n",
            "duration_stats = audio_processor.process_audio_clips(soa_test_full_clips, TEST_DIR)\n",
            "print(duration_stats)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 4. Mean and standard deviation of training dataset\n",
            "It is important to normalize data for our Neural Networks. It creates better distribution. It should smoothen loss function plane (so it would be easier to find global minimum). We use only training dataset for this purpose and save it to JSON file next to datasets' image directories.\n",
            "\n",
            "Effect of normalization for example CNN can be seen [here](https://wandb.ai/mytkom-warsaw-university-of-technology/iml/reports/Effect-of-normalization-input-for-TutorialCNN--VmlldzoxMDUxMTI1OQ?accessToken=s67utpfjryb4um1240bd56i51zo5oy2bj0gbaqqz79z3hnnabkub1rdhsamhwd2v)."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Mean: 0.4177287220954895, Standard deviation: 0.17890231311321259\n"
               ]
            }
         ],
         "source": [
            "mean, std = compute_mean_std_from_images(TRAIN_DIR)\n",
            "print(f\"Mean: {mean}, Standard deviation: {std}\")\n",
            "save_mean_std(mean, std, f\"{DATASET_DIR}/scaling_params.json\")"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.9.6"
      },
      "vscode": {
         "interpreter": {
            "hash": "cccc1e4ed51fc8b9fd510a52ec83f3ba6504ae6c1f28f1731113cc11ad46be9d"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
