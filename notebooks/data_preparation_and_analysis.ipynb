{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## First, run this cell to set up paths and import dependencies"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import os\n",
            "\n",
            "from tqdm import tqdm\n",
            "\n",
            "if not os.path.exists(r\"./notebooks\"):\n",
            "    %cd ..\n",
            "\n",
            "from src.audio_processor import AudioProcessor\n",
            "from src.audio_dataset_processor import DAPSDatasetProcessor\n",
            "from src.data_processing import SOAAudioClips, save_mean_std, compute_mean_std_from_images\n",
            "from src.dataset_analysis import duration_statistics\n",
            "from src.config import VALID_ACCESS_LABELS, TRAIN_DIR, TEST_DIR, VAL_DIR, DATA_DIR, DATASET_DIR\n",
            "\n",
            "os.makedirs(DATASET_DIR, exist_ok=True)\n",
            "os.makedirs(TRAIN_DIR, exist_ok=True)\n",
            "os.makedirs(VAL_DIR, exist_ok=True)\n",
            "os.makedirs(TEST_DIR, exist_ok=True)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 1. Split all allowed .wav files\n",
            "We are using [DAPS](https://zenodo.org/records/4660670) dataset. It has several directories available in which there are .wav files of 5 scripts read by 20 speakers. Directories differ from each other with augmentation, which is labeled by `room` and `recording device`. In this cell we are specifying allowed directories, their contents are being discovered and splitted into 3 datasets (training, validation and test). The same script cannot be in the same dataset - `AudioDatasetProcessor` class take care of that. Balancing classes is done using batch_sampler in DataLoader by undersampling major class."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "allowed_directories=['ipadflat_confroom1', 'ipadflat_office1', 'ipad_balcony1', 'ipad_bedroom1', 'ipad_confroom1', 'ipad_confroom2', 'ipad_livingroom1', 'ipad_office1', 'ipad_office2', 'iphone_balcony1', 'iphone_bedroom1', 'iphone_livingroom1']\n",
            "dataset_processor = DAPSDatasetProcessor(DATA_DIR, VALID_ACCESS_LABELS, allowed_directories)\n",
            "dataset_processor.compute_statistics()\n",
            "train_set, validate_set, test_set = dataset_processor.get_datasets()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 2. Duration statistics of .wav files\n",
            "We first are checking full clips duration statistics."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "soa_train_full_clips = SOAAudioClips(train_set)\n",
            "soa_test_full_clips = SOAAudioClips(validate_set)\n",
            "soa_val_full_clips = SOAAudioClips(test_set)\n",
            "\n",
            "print(\"\\nDataset Statistics:\")\n",
            "print(\"Training set:\")\n",
            "print(duration_statistics(soa_train_full_clips.clips))\n",
            "\n",
            "print(\"Validation set:\")\n",
            "print(duration_statistics(soa_test_full_clips.clips))\n",
            "\n",
            "print(\"Test set:\")\n",
            "print(duration_statistics(soa_val_full_clips.clips))"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 3. Split into few seconds clips\n",
            "Now we decide to split full clips into few seconds subclips, they are filtered so only clips with more than `0.5` of recording has speech detected by `webrtcvad`. We are doing this to establish the same size of input for CNN."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Then after processing we save log mel grayscale spectrograms into separate directories for every dataset."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "audio_processor = AudioProcessor()\n",
            "\n",
            "print(\"Preprocessed Train Dataset:\")\n",
            "duration_stats = audio_processor.process_audio_clips(soa_train_full_clips, TRAIN_DIR)\n",
            "print(duration_stats)\n",
            "\n",
            "print(\"\\nPreprocessed Validation Dataset:\")\n",
            "duration_stats = audio_processor.process_audio_clips(soa_val_full_clips, VAL_DIR)\n",
            "print(duration_stats)\n",
            "\n",
            "print(\"\\nPreprocessed Test Dataset:\")\n",
            "duration_stats = audio_processor.process_audio_clips(soa_test_full_clips, TEST_DIR)\n",
            "print(duration_stats)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 4. Mean and standard deviation of training dataset\n",
            "It is important to normalize data for our Neural Networks. It creates better distribution. It should smoothen loss function plane (so it would be easier to find global minimum). We use only training dataset for this purpose and save it to JSON file next to datasets' image directories.\n",
            "\n",
            "Effect of normalization for example CNN can be seen [here](https://wandb.ai/mytkom-warsaw-university-of-technology/iml/reports/Effect-of-normalization-input-for-TutorialCNN--VmlldzoxMDUxMTI1OQ?accessToken=s67utpfjryb4um1240bd56i51zo5oy2bj0gbaqqz79z3hnnabkub1rdhsamhwd2v)."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "mean, std = compute_mean_std_from_images(TRAIN_DIR)\n",
            "print(f\"Mean: {mean}, Standard deviation: {std}\")\n",
            "save_mean_std(mean, std, f\"{DATASET_DIR}/scaling_params.json\")"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.8"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}