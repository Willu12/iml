{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up paths and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "if not os.path.exists(\"./notebooks\"):\n",
    "    %cd ..\n",
    "\n",
    "import src.model\n",
    "from src.training import train, validate\n",
    "from src.config import VALID_ACCESS_LABELS, TRAIN_DIR, TEST_DIR, VAL_DIR\n",
    "\n",
    "wandb_enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    learning_rate = 0.005\n",
    "    epochs = 40\n",
    "    batch_size = 32\n",
    "    image_size = (32, 32) # TODO: choose best image_size\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally initialize W&B project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"iml\", config=vars(config))\n",
    "wandb_enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a custom dataset class to load spectrograms from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.png')]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        speaker_id = img_path.split('/')[-1].split('_')[0]\n",
    "        label = int(speaker_id in VALID_ACCESS_LABELS)\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set up data transformations and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(config.image_size),\n",
    "    transforms.ToTensor(),\n",
    "    # TODO: normalization\n",
    "])\n",
    "\n",
    "train_dataset = SpectrogramDataset(TRAIN_DIR, transform=transform)\n",
    "val_dataset = SpectrogramDataset(VAL_DIR, transform=transform)\n",
    "test_dataset = SpectrogramDataset(TEST_DIR, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize model, optimizer and set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = src.model.SimpleCNN().to(device)\n",
    "model = src.model.TutorialCNN().to(device) # - for 32x32 images\n",
    "model.device = device\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(config.epochs):\n",
    "    print(f\"Epoch {epoch+1}/{config.epochs}\")\n",
    "\n",
    "    if wandb_enabled:\n",
    "        logger = wandb.log\n",
    "    else:\n",
    "        logger = lambda data,step: print(f\"  Step {step}: {data}\")\n",
    "\n",
    "    train(model, train_loader, criterion, optimizer, epoch, logger, len(train_loader) // 5 - 1)\n",
    "    # 0 for recall and precision in first few epochs is expected (case when one class wasn't predicted yet)\n",
    "    metrics = validate(model, val_loader)\n",
    "    print(metrics)\n",
    "\n",
    "    if wandb_enabled:\n",
    "        wandb.log({\"validation/recall\": metrics.recall, \"validation/precision\": metrics.precision, \"validation/f1\": metrics.f1, \"epoch\": epoch+1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = \"./models/simple_cnn.pth\"\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "if wandb_enabled: \n",
    "    wandb.save(model_path)\n",
    "    wandb.finish()\n",
    "\n",
    "print(\"Training complete and model saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
