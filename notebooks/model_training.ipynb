{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up paths and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "import src.model\n",
    "from src.training import train, validate\n",
    "from src.config import VALID_ACCESS_LABELS, TRAIN_DIR, TEST_DIR, VAL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize W&B project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"iml\", config={\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"epochs\": 40,\n",
    "    \"batch_size\": 32,\n",
    "    \"image_size\": (32, 32) # TODO: choose best image_size\n",
    "})\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a custom dataset class to load spectrograms from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.png')]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        speaker_id = img_path.split('/')[-1].split('_')[0]\n",
    "        label = int(speaker_id in VALID_ACCESS_LABELS)\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set up data transformations and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(config.image_size),\n",
    "    transforms.ToTensor(),\n",
    "    # TODO: normalization\n",
    "])\n",
    "\n",
    "train_dataset = SpectrogramDataset(TRAIN_DIR, transform=transform)\n",
    "val_dataset = SpectrogramDataset(VAL_DIR, transform=transform)\n",
    "test_dataset = SpectrogramDataset(TEST_DIR, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize model, optimizer and set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = src.model.SimpleCNN().to(device)\n",
    "model = src.model.TutorialCNN() # - for 32x32 images\n",
    "model.device = device\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(config.epochs):\n",
    "    print(f\"Epoch {epoch+1}/{config.epochs}\")\n",
    "    \n",
    "    train(model, train_loader, criterion, optimizer, epoch, wandb.log, len(train_loader) // 5 - 1)\n",
    "    # 0 for recall and precision in first few epochs is expected (case when one class wasn't predicted yet)\n",
    "    recall, precision, val_f1 = validate(model, val_loader)\n",
    "    wandb.log({\"validation/recall\": recall, \"validation/precision\": precision, \"validation/f1\": val_f1, \"epoch\": epoch+1})\n",
    "    print(f\"Recall: {recall:.2f}, Precision: {precision:.2f}, F1: {val_f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = \"./models/simple_cnn.pth\"\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "torch.save(model.state_dict(), model_path)\n",
    "wandb.save(model_path)\n",
    "wandb.finish()\n",
    "\n",
    "print(\"Training complete and model saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
