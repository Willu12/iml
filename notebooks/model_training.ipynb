{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Set up paths and imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "import torch\n",
                "from torchvision import transforms\n",
                "\n",
                "if not os.path.exists(\"./notebooks\"):\n",
                "    %cd ..\n",
                "\n",
                "import src.model\n",
                "from src.training import do_train, do_test\n",
                "from src.dataset import prepare_dataset_loaders\n",
                "from src.data_processing import load_mean_std\n",
                "from src.config import DATASET_DIR\n",
                "\n",
                "wandb_enabled = False"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load standarization data and define Config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mean, std = load_mean_std(f\"{DATASET_DIR}/scaling_params.json\")\n",
                "\n",
                "class Config:\n",
                "    def __init__(self, lr=0.001, epochs=40, batch_size=32):\n",
                "        self.learning_rate = lr\n",
                "        self.epochs = epochs\n",
                "        self.batch_size = batch_size"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Optionally initialize W&B project"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wandb_enabled = True"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Choose device"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Choose your architecture"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "name = \"TutorialCNN\"\n",
                "model = src.model.TutorialCNN()\n",
                "config = Config()\n",
                "transform = transforms.Compose([\n",
                "    transforms.Resize((32,32)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean, std)\n",
                "])\n",
                "train_loader, val_loader, test_loader = prepare_dataset_loaders(transform, config.batch_size)\n",
                "criterion = torch.nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
                "\n",
                "run = do_train(name, train_loader, val_loader, config, model, criterion, optimizer, device, wandb_enabled)\n",
                "do_test(name, test_loader, model.__class__, {}, run.id, device, wandb_enabled)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "name = \"OriginalSizeCNN\"\n",
                "model = src.model.OriginalSizeCNN()\n",
                "config = Config()\n",
                "transform = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean, std)\n",
                "])\n",
                "train_loader, val_loader, test_loader = prepare_dataset_loaders(transform, config.batch_size)\n",
                "criterion = torch.nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
                "\n",
                "#run = do_train(name, train_loader, val_loader, config, model, criterion, optimizer, device, wandb_enabled)\n",
                "do_test(name, test_loader, model.__class__, {}, run.id, device, wandb_enabled)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "name = \"ResNetBatchNormalization\"\n",
                "model_args={\n",
                "    \"batch_normalization\": True,\n",
                "    \"residual_connections\": True\n",
                "}\n",
                "model = src.model.OurResNet(**model_args)\n",
                "config = Config()\n",
                "transform = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean, std)\n",
                "])\n",
                "train_loader, val_loader, test_loader = prepare_dataset_loaders(transform, config.batch_size)\n",
                "criterion = torch.nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
                "\n",
                "run = do_train(name, train_loader, val_loader, config, model, criterion, optimizer, device, wandb_enabled)\n",
                "do_test(name, test_loader, model.__class__, model_args, run.id, device, wandb_enabled)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "name = \"DropoutCNN\"\n",
                "model = src.model.DropoutCNN()\n",
                "config = Config()\n",
                "transform = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean, std)\n",
                "])\n",
                "train_loader, val_loader, test_loader = prepare_dataset_loaders(transform, config.batch_size)\n",
                "criterion = torch.nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
                "\n",
                "run = do_train(name, train_loader, val_loader, config, model, criterion, optimizer)\n",
                "\n",
                "do_test(name, test_loader, model.__class__, run, device, wandb_enabled)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "ensemble_models = []\n",
                "for i in range(10):\n",
                "    model = src.model.OriginalSizeCNN()\n",
                "    config = Config()\n",
                "    transform = transforms.Compose([\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize(mean, std)\n",
                "    ])\n",
                "    torch.manual_seed(i)\n",
                "    np.random.seed(i)\n",
                "    train_loader, val_loader, test_loader = prepare_dataset_loaders(transform, config.batch_size)\n",
                "    criterion = torch.nn.CrossEntropyLoss()\n",
                "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
                "\n",
                "    run = do_train(name, train_loader, val_loader, config, model, criterion, optimizer, device, wandb_enabled)\n",
                "    do_test(name, test_loader, model.__class__, run, device, wandb_enabled)\n",
                "    ensemble_models.append(model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ensemble_model_names = [\"OriginalSizeCNN-HE-RELU\", \"OriginalSizeCNN-UNIFORM-RELU\", \"OriginalSizeCNN-XAVIER-RELU\"]\n",
                "ensemble_models = []\n",
                "for model_name in ensemble_model_names:\n",
                "    model = src.model.OriginalSizeCNN()\n",
                "    model.load_state_dict(torch.load(f\"./models/{model_name}.pth\", weights_only=True))\n",
                "    model.device = device\n",
                "    model.to(device)\n",
                "    ensemble_models.append(model)\n",
                "\n",
                "name = \"EnsembleCNN\"\n",
                "model = src.model.EnsembleCNN(ensemble_models, 2)\n",
                "config = Config()\n",
                "transform = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean, std)\n",
                "])\n",
                "train_loader, val_loader, test_loader = prepare_dataset_loaders(transform, config.batch_size)\n",
                "criterion = torch.nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
                "\n",
                "run = do_train(name, train_loader, val_loader, config, model, criterion, optimizer)\n",
                "do_test(name, test_loader, model.__class__, run, device, wandb_enabled)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Comparison of our CNN architectures\n",
                "Comparison of architectures trainable using this notebook can be seen [here](https://wandb.ai/mytkom-warsaw-university-of-technology/iml/reports/Comparison-of-from-scratch-architectures--VmlldzoxMDU0MDk4NQ?accessToken=mle3zdqu8bxvrc4z8pdhl89talltdlml5gw5zmictx9e0qhvue0k5awsdggr37vp).\n",
                "\n",
                "`TutorialCNN` demonstrated considerably lower validation and test accuracy as well as F1 scores. It also converged much more slowly. In contrast, `OriginalSizeCNN` and `ResNetBatchNormalization` exhibited similar performance, with `ResNetBatchNormalization` converging slightly faster. We believe the lack of a significant difference between their performances is due to the simplicity of the classification task."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Appendix A. Impact of Data Normalization on Neural Network Performance\n",
                "\n",
                "This section introduces an alternative configuration of the `TutorialCNN` neural network, where data normalization is omitted. All other settings and hyperparameters remain unchanged. The results of the performance comparison between the normalized and non-normalized datasets using `TutorialCNN` can be found [here](https://api.wandb.ai/links/mytkom-warsaw-university-of-technology/wj0f1okh)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "name = \"TutorialCNN without standardization\"\n",
                "model = src.model.TutorialCNN()\n",
                "config = Config()\n",
                "transform = transforms.Compose([\n",
                "    transforms.Resize((32,32)),\n",
                "    transforms.ToTensor()\n",
                "])\n",
                "train_loader, val_loader, test_loader = prepare_dataset_loaders(transform, config.batch_size)\n",
                "criterion = torch.nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
                "\n",
                "run = do_train(name, train_loader, val_loader, config, model, criterion, optimizer, device, wandb_enabled)\n",
                "do_test(name, test_loader, model.__class__, {}, run.id, device, wandb_enabled)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Appendix B. Impact of Skip Connections and Batch Normalization in Residual Networks\n",
                "\n",
                "In this section, we delve into the significance of **residual addition (skip connections)** and **batch normalization** in training deep convolutional neural networks, particularly focusing on their role within the `OurResNet` architecture. To demonstrate their impact, we compare three training configurations:\n",
                "\n",
                "1. **`NotResNet`:** A baseline architecture without skip connections or batch normalization. This configuration employs only convolutional and pooling layers, which limits its ability to mitigate vanishing gradients and optimize deeper architectures effectively.\n",
                "\n",
                "2. **`ResNetNoBatchNormalization`:** This version incorporates skip connections but excludes batch normalization. It demonstrates the advantage of residual addition while highlighting the challenges posed by the lack of batch normalization.\n",
                "\n",
                "3. **`ResNetBatchNormalization` (default configuration):** This configuration includes both skip connections and batch normalization, showcasing the synergistic effect of these features on performance.\n",
                "\n",
                "### Role of Skip Connections (Residual Addition)\n",
                "\n",
                "Skip connections are central to residual networks, enabling the output of a layer to \"skip\" over one or more subsequent layers and be added to the output of a deeper layer. This technique facilitates:\n",
                "- **Gradient flow:** During backpropagation, skip connections ensure that gradients can bypass intermediate layers, reducing the risk of vanishing gradients.\n",
                "- **Optimization stability:** They provide a direct pathway for information, simplifying optimization for deeper networks.\n",
                "- **Feature reuse:** By combining learned features with earlier representations, skip connections enable the network to refine its understanding without losing essential information.\n",
                "\n",
                "In the `OurResNet` architecture, each residual block optionally applies residual addition. When enabled, the network adds the input of the block (possibly downsampled) to the output of the final convolutional layer in that block, ensuring efficient information propagation.\n",
                "\n",
                "### Role of Batch Normalization\n",
                "\n",
                "Batch normalization normalizes the activations of a layer for each mini-batch during training. This process has multiple benefits:\n",
                "- **Faster convergence:** By reducing internal covariate shift, batch normalization allows the model to converge faster.\n",
                "- **Regularization:** It introduces slight noise during training, which acts as a regularizer and reduces the risk of overfitting.\n",
                "- **Stabilized training:** By keeping the activation values within a standardized range, batch normalization improves the overall stability of the training process.\n",
                "\n",
                "In `OurResNet`, batch normalization is applied after each convolutional layer within the residual blocks (when enabled). It helps maintain consistent activation distributions, even as the network grows deeper.\n",
                "\n",
                "\n",
                "### Results and Comparison\n",
                "\n",
                "A detailed comparison of these configurations' performance is available in the following **Weights & Biases (wandb) report**:  \n",
                "[Link to wandb report](https://wandb.ai/mytkom-warsaw-university-of-technology/iml/reports/Impact-of-Skip-Connections-and-Batch-Normalization-in-Residual-Networks--VmlldzoxMTEwNDI2MQ?accessToken=kbbga0avcuq4pimsxfd0quvdtuay3mqfkhudfkdkwj6yoosy3srq138d5s06np7v).\n",
                "\n",
                "In our experiment, turning on/off batch normalization and skipping connections showed little differences in training convergence. We believe the lack of a substantial difference between their training performances is due to the simplicity of the classification task and too shallow neural network. Also, the use of Adam optimizer probably helped NotResNet to converge faster."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "name = \"NotResNet\"\n",
                "model_args={\n",
                "    \"batch_normalization\": False,\n",
                "    \"residual_connections\": False\n",
                "}\n",
                "model = src.model.OurResNet(**model_args)\n",
                "config = Config()\n",
                "transform = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean, std)\n",
                "])\n",
                "train_loader, val_loader, test_loader = prepare_dataset_loaders(transform, config.batch_size)\n",
                "criterion = torch.nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
                "\n",
                "run = do_train(name, train_loader, val_loader, config, model, criterion, optimizer, device, wandb_enabled)\n",
                "do_test(name, test_loader, model.__class__, model_args, run.id, device, wandb_enabled)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "name = \"ResNetNoBatchNormalization\"\n",
                "model_args={\n",
                "    \"batch_normalization\": False,\n",
                "    \"residual_connections\": True\n",
                "}\n",
                "model = src.model.OurResNet(**model_args)\n",
                "config = Config()\n",
                "transform = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean, std)\n",
                "])\n",
                "train_loader, val_loader, test_loader = prepare_dataset_loaders(transform, config.batch_size)\n",
                "criterion = torch.nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
                "\n",
                "run = do_train(name, train_loader, val_loader, config, model, criterion, optimizer, device, wandb_enabled)\n",
                "do_test(name, test_loader, model.__class__, model_args, run.id, device, wandb_enabled)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We suspected that Adam optimizer reduces impact of batch normalization and skip connections on training performance. So we performed similar experiment using Stochastic Gradient Descent with momentum of 0.9 with 10 times bigger learning rate (0.01). The results are presented [here](https://wandb.ai/mytkom-warsaw-university-of-technology/iml/reports/SGD-Impact-of-Skip-Connections-and-Batch-Normalization-in-Residual-Networks--VmlldzoxMTEwNjI2Ng?accessToken=ronkgh3c8etvumnzmcettjrbfbjnkwosuq5ownyt7vzjxko44ous49ecb3oaspmj).\n",
                "\n",
                "Compared to Adam, using SGD as an optimizer shows significant differences between networks with skip connections and those without it. The addition of batch normalization added another slight improvement for training convergence. If the neural network was deeper and the classification task harder, we suspect more significant differences (even NotResNetSGD not training at all, locked in some local minimum of the loss function)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "name = \"NotResNetSGD\"\n",
                "model_args={\n",
                "    \"batch_normalization\": False,\n",
                "    \"residual_connections\": False\n",
                "}\n",
                "model = src.model.OurResNet(**model_args)\n",
                "config = Config(lr=0.01)\n",
                "transform = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean, std)\n",
                "])\n",
                "train_loader, val_loader, test_loader = prepare_dataset_loaders(transform, config.batch_size)\n",
                "criterion = torch.nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.SGD(model.parameters(), lr=config.learning_rate, momentum=0.9)\n",
                "\n",
                "run = do_train(name, train_loader, val_loader, config, model, criterion, optimizer, device, wandb_enabled)\n",
                "do_test(name, test_loader, model.__class__, model_args, run.id, device, wandb_enabled)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "name = \"ResNetNoBatchNormalizationSGD\"\n",
                "model_args={\n",
                "    \"batch_normalization\": False,\n",
                "    \"residual_connections\": True\n",
                "}\n",
                "model = src.model.OurResNet(**model_args)\n",
                "config = Config(lr=0.01)\n",
                "transform = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean, std)\n",
                "])\n",
                "train_loader, val_loader, test_loader = prepare_dataset_loaders(transform, config.batch_size)\n",
                "criterion = torch.nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.SGD(model.parameters(), lr=config.learning_rate, momentum=0.9)\n",
                "\n",
                "run = do_train(name, train_loader, val_loader, config, model, criterion, optimizer, device, wandb_enabled)\n",
                "do_test(name, test_loader, model.__class__, model_args, run.id, device, wandb_enabled)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "name = \"ResNetBatchNormalizationSGD\"\n",
                "model_args={\n",
                "    \"batch_normalization\": True,\n",
                "    \"residual_connections\": True\n",
                "}\n",
                "model = src.model.OurResNet(**model_args)\n",
                "config = Config(lr=0.01)\n",
                "transform = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean, std)\n",
                "])\n",
                "train_loader, val_loader, test_loader = prepare_dataset_loaders(transform, config.batch_size)\n",
                "criterion = torch.nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.SGD(model.parameters(), lr=config.learning_rate, momentum=0.9)\n",
                "\n",
                "run = do_train(name, train_loader, val_loader, config, model, criterion, optimizer, device, wandb_enabled)\n",
                "do_test(name, test_loader, model.__class__, model_args, run.id, device, wandb_enabled)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.6"
        },
        "vscode": {
            "interpreter": {
                "hash": "cccc1e4ed51fc8b9fd510a52ec83f3ba6504ae6c1f28f1731113cc11ad46be9d"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
