{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Set up paths and imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "import torch\n",
                "from torchvision import transforms\n",
                "\n",
                "if not os.path.exists(\"./notebooks\"):\n",
                "    %cd ..\n",
                "\n",
                "import src.model\n",
                "from src.training import do_train, do_test\n",
                "from src.dataset import prepare_dataset_loaders\n",
                "from src.data_processing import load_mean_std\n",
                "from src.config import DATASET_DIR\n",
                "\n",
                "wandb_enabled = False"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load standarization data and define Config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mean, std = load_mean_std(f\"{DATASET_DIR}/scaling_params.json\")\n",
                "\n",
                "class Config:\n",
                "    def __init__(self, lr=0.001, epochs=40, batch_size=32):\n",
                "        self.learning_rate = lr\n",
                "        self.epochs = epochs\n",
                "        self.batch_size = batch_size"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Optionally initialize W&B project"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "wandb_enabled = True"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Choose device"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Choose your architecture"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "name = \"TutorialCNN without standardization\"\n",
                "model = src.model.TutorialCNN()\n",
                "config = Config()\n",
                "transform = transforms.Compose([\n",
                "    transforms.Resize((32,32)),\n",
                "    transforms.ToTensor()\n",
                "])\n",
                "train_loader, val_loader, test_loader = prepare_dataset_loaders(transform, config.batch_size)\n",
                "criterion = torch.nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
                "\n",
                "run = do_train(name, train_loader, val_loader, config, model, criterion, optimizer, device, wandb_enabled)\n",
                "do_test(name, test_loader, model.__class__, run, device, wandb_enabled)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "name = \"TutorialCNN\"\n",
                "model = src.model.TutorialCNN()\n",
                "config = Config()\n",
                "transform = transforms.Compose([\n",
                "    transforms.Resize((32,32)),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean, std)\n",
                "])\n",
                "train_loader, val_loader, test_loader = prepare_dataset_loaders(transform, config.batch_size)\n",
                "criterion = torch.nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
                "\n",
                "run = do_train(name, train_loader, val_loader, config, model, criterion, optimizer, device, wandb_enabled)\n",
                "do_test(name, test_loader, model.__class__, run, device, wandb_enabled)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "name = \"OriginalSizeCNN\"\n",
                "model = src.model.OriginalSizeCNN()\n",
                "config = Config()\n",
                "transform = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean, std)\n",
                "])\n",
                "train_loader, val_loader, test_loader = prepare_dataset_loaders(transform, config.batch_size)\n",
                "criterion = torch.nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
                "\n",
                "run = do_train(name, train_loader, val_loader, config, model, criterion, optimizer, device, wandb_enabled)\n",
                "do_test(name, test_loader, model.__class__, run, device, wandb_enabled)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "name = \"DropoutCNN\"\n",
                "model = src.model.DropoutCNN()\n",
                "config = Config()\n",
                "transform = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean, std)\n",
                "])\n",
                "train_loader, val_loader, test_loader = prepare_dataset_loaders(transform, config.batch_size)\n",
                "criterion = torch.nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
                "\n",
                "do_train(name, train_loader, val_loader, config, model, criterion, optimizer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "ensemble_model_names = [\"OriginalSizeCNN-HE-RELU\", \"OriginalSizeCNN-UNIFORM-RELU\", \"OriginalSizeCNN-XAVIER-RELU\"]\n",
                "\n",
                "ensemble_models = []\n",
                "for model_name in ensemble_model_names:\n",
                "    model = src.model.OriginalSizeCNN()\n",
                "    model.load_state_dict(torch.load(f\"./models/{model_name}.pth\", weights_only=True))\n",
                "    model.device = device\n",
                "    model.to(device)\n",
                "    ensemble_models.append(model)\n",
                "\n",
                "name = \"EnsembleCNN\"\n",
                "model = src.model.EnsembleCNN(ensemble_models, 2)\n",
                "config = Config()\n",
                "transform = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean, std)\n",
                "])\n",
                "train_loader, val_loader, test_loader = prepare_dataset_loaders(transform, config.batch_size)\n",
                "criterion = torch.nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
                "\n",
                "do_train(name, train_loader, val_loader, config, model, criterion, optimizer)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Comparison of models\n",
                "Comparison of architectures trainable using this notebook can be seen [here](https://wandb.ai/mytkom-warsaw-university-of-technology/iml/reports/Comparison-of-from-scratch-architectures--VmlldzoxMDU0MDk4NQ?accessToken=mle3zdqu8bxvrc4z8pdhl89talltdlml5gw5zmictx9e0qhvue0k5awsdggr37vp)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
