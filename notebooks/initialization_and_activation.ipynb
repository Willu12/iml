{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN initialization, activation and optimization\n",
    "\n",
    "This notebook demonstrates the performance of CNNs with different configurations.\n",
    "\n",
    "In particular it explores the Kaiming-He, Xavier, uniform and PyTorch default weight initialization. It also utilizes activation functions such as relu and sigmoid and compares Adam and SDG optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up paths and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "if not os.path.exists(\"./notebooks\"):\n",
    "    %cd ..\n",
    "\n",
    "import src.model\n",
    "from src.training import do_train, do_test\n",
    "from src.dataset import prepare_dataset_loaders\n",
    "from src.data_processing import load_mean_std\n",
    "from src.config import DATASET_DIR\n",
    "\n",
    "wandb_enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load standarization data and define Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = load_mean_std(f\"{DATASET_DIR}/scaling_params.json\")\n",
    "\n",
    "he = lambda m: torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "xavier = lambda m: torch.nn.init.xavier_uniform_(m.weight)\n",
    "uniform = lambda m: torch.nn.init.uniform_(m.weight)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, lr=0.001, epochs=40, batch_size=32):\n",
    "        self.learning_rate = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally initialize W&B project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choose device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose your architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration below proved to be the best overall and achieved the fastest convergence.\n",
    "\n",
    "Almost all other configurations presented in this notebook, while slightly worse, still achieved similar performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"INIT:PyTorch-ACT:Relu-OPT:Adam-LR:0.001\"\n",
    "model = src.model.OriginalSizeCNN(\n",
    "    initialize=None,\n",
    "    activation=F.relu,\n",
    ")\n",
    "config = Config(\n",
    "    lr=0.001,\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switching activation on the final layer to Sigmoid visibly slows down convergence.\n",
    "\n",
    "However, the results after 50 epochs are extremely close to Relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"INIT:PyTorch-ACT:Sigmoid-OPT:Adam-LR:0.001\"\n",
    "model = src.model.OriginalSizeCNN(\n",
    "    initialize=None,\n",
    "    activation=F.sigmoid,\n",
    ")\n",
    "config = Config(\n",
    "    lr=0.001,\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration converges very slowly due to the combination of SDG optimizer and learning rate of 0.001.\n",
    "\n",
    "It's worth noting that the same learning rate yields good results when used with Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"INIT:PyTorch-ACT:Sigmoid-OPT:SDG-LR:0.001\"\n",
    "model = src.model.OriginalSizeCNN(\n",
    "    initialize=None,\n",
    "    activation=F.sigmoid,\n",
    ")\n",
    "config = Config(\n",
    "    lr=0.001,\n",
    ")\n",
    "optimizer = torch.optim.SDG(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four configurations presented below all show very similar performance, proving that for a simple dataset, there is almost no difference in choice of initialization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"INIT:PyTorch-ACT:Sigmoid-OPT:SDG-LR:0.05\"\n",
    "model = src.model.OriginalSizeCNN(\n",
    "    initialize=None,\n",
    "    activation=F.sigmoid,\n",
    ")\n",
    "config = Config(\n",
    "    lr=0.05,\n",
    ")\n",
    "optimizer = torch.optim.SDG(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"INIT:He-ACT:Sigmoid-OPT:SDG-LR:0.05\"\n",
    "model = src.model.OriginalSizeCNN(\n",
    "    initialize=he,\n",
    "    activation=F.sigmoid,\n",
    ")\n",
    "config = Config(\n",
    "    lr=0.05,\n",
    ")\n",
    "optimizer = torch.optim.SDG(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"INIT:Xavier-ACT:Sigmoid-OPT:SDG-LR:0.05\"\n",
    "model = src.model.OriginalSizeCNN(\n",
    "    initialize=xavier,\n",
    "    activation=F.sigmoid,\n",
    ")\n",
    "config = Config(\n",
    "    lr=0.05,\n",
    ")\n",
    "optimizer = torch.optim.SDG(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"INIT:Uniform-ACT:Sigmoid-OPT:SDG-LR:0.05\"\n",
    "model = src.model.OriginalSizeCNN(\n",
    "    initialize=he,\n",
    "    activation=F.sigmoid,\n",
    ")\n",
    "config = Config(\n",
    "    lr=0.05,\n",
    ")\n",
    "optimizer = torch.optim.SDG(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last configuration struggles to converge and achieves very poor results. The cause lies in increased learning rate of 0.05. While this learning rate is optimal for SDG is seems to result in high instability for Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"INIT:PyTorch-ACT:Sigmoid-OPT:Adam-LR:0.05\"\n",
    "model = src.model.OriginalSizeCNN(\n",
    "    initialize=None,\n",
    "    activation=F.sigmoid,\n",
    ")\n",
    "config = Config(\n",
    "    lr=0.05,\n",
    ")\n",
    "optimizer = torch.optim.SDG(model.parameters(), lr=config.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = prepare_dataset_loaders(transform, config.batch_size)\n",
    "run = do_train(name, train_loader, val_loader, config, model, criterion, optimizer, device, wandb_enabled)\n",
    "do_test(name, test_loader, model.__class__, run, device, wandb_enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of models\n",
    "Comparison of architectures trainable using this notebook can be seen [here](https://api.wandb.ai/links/patonymous-warsaw-university-of-technology/sajmu7qa).\n",
    "\n",
    "Almost all CNNs presented here achieved around 0.9 validation accuracy within 50 epochs. The notable exception is the combination of SDG optimizer with learning rate of 0.001 which has shown very slow convergence. The fastest when it comes to learning and the best overall proved to be the combination of Relu activation, PyTorch default initialization and Adam optimizer with learning rate equal 0.001.\n",
    "\n",
    "The differences between different initialization methods are insignificant after few initial epochs. It seems that for a simple classification problem such as this one, it's possible to find multiple well-performing configurations. Adam with Relu and learning rate of 0.001 achieves similarly good results as SDG with Sigmoid and learning rate of 0.05.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
