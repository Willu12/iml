{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e69269f0",
   "metadata": {},
   "source": [
    "# Semantic image search\n",
    "In this notebook we will try to find the semanting image search, we will focus on feature extraction and semantic similarity search."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9dc586d",
   "metadata": {},
   "source": [
    "## Overview of semantic image search\n",
    "Semantic image search is the task of ranking a collection of images based on their similarity to a given query image. While there are various ways to approach this problem, the general workflow of our solution is both intuitive and effective, involving the following steps:\n",
    "\n",
    "\n",
    "![image info](./images/semantic_search.png)\n",
    "\n",
    "The process begins with extracting feature maps from each image in the dataset using a pre-trained convolutional neural network (CNN) OriginalSizeCNN. These feature maps serve as a compact representation of each image’s visual content. Once we have these feature maps, we calculate the similarity between images by measuring the distance between their feature vectors. This process is repeated for every image pair in the dataset, resulting in a list of similarity scores. By sorting these scores in descending order, we can rank images by how similar they are to the query image.\n",
    "\n",
    "In essence, a well-trained model should produce feature maps that capture semantic similarity—meaning the feature maps of similar images are closer together in vector space, while dissimilar images are farther apart. The ultimate goal is for the model to learn representations that meaningfully reflect how humans perceive image similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beb56e8594def0c",
   "metadata": {},
   "source": [
    "## Set up paths and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not os.path.exists(\"./notebooks\"):\n",
    "    %cd ..\n",
    "\n",
    "import src.model\n",
    "from PIL import Image\n",
    "from src.data_processing import load_mean_std\n",
    "from src.config import DATASET_DIR\n",
    "from src.dataset_analysis import plot_spectrogram\n",
    "from src.training import process_predictions_and_features\n",
    "from src.dataset import prepare_dataset_loader\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "336fd973",
   "metadata": {},
   "source": [
    "## 0. Set device and extact the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edea260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def copy_images_with_keywords(src_dir, dest_dir, keywords):\n",
    "    if os.path.exists(dest_dir):\n",
    "        return\n",
    "    \n",
    "    os.makedirs(dest_dir)\n",
    "\n",
    "    for filename in os.listdir(src_dir):\n",
    "        if any(keyword.lower() in filename.lower() for keyword in keywords): \n",
    "            src_path = os.path.join(src_dir, filename)\n",
    "            dest_path = os.path.join(dest_dir, filename)\n",
    "            \n",
    "            if os.path.isfile(src_path) and filename.lower().endswith(('jpg', 'jpeg', 'png', 'gif', 'bmp')):\n",
    "                shutil.copy(src_path, dest_path)\n",
    "\n",
    "allowed_directories=['ipadflat_confroom1', 'ipadflat_office1', 'ipad_balcony1', 'ipad_bedroom1', 'ipad_confroom1']\n",
    "copy_images_with_keywords(\"datasets/test\", \"feature_datasets\", allowed_directories)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db708b17",
   "metadata": {},
   "source": [
    "## 1. Load Images from dataset\n",
    "In the CNN visual interpretation notebook, we used only a subset of the dataset for analysis. While the models were trained on the complete dataset, performing feature extraction on the entire dataset required excessive storage. To address this, we reduced the dataset to include only the following files: `ipadflat_confroom1`, `ipadflat_office1`, `ipad_balcony1`, `ipad_bedroom1`, and `ipad_confroom1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4579391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(directory, transform):\n",
    "    images = []\n",
    "    image_files = sorted(os.listdir(directory))  \n",
    "    for filename in image_files:\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image = Image.open(filepath).convert(\"L\")\n",
    "            image = transform(image)\n",
    "            images.append(image)\n",
    "    return images, image_files\n",
    "\n",
    "mean, std = load_mean_std(f\"{DATASET_DIR}/scaling_params.json\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "feature_dataset_directory = \"feature_datasets\"\n",
    "images, image_files = load_images(feature_dataset_directory, transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9acf38b730cb26f9",
   "metadata": {},
   "source": [
    "## 2. Load OriginalSizeCNN model\n",
    "In this section, we present the visual interpretation of our most successful model, the OriginalSizeCNN. This model comprises three convolutional layers, from which we aim to extract and analyze feature maps.\n",
    "\n",
    "Feature maps, the outputs of these convolutional layers, represent the spatial patterns and features detected by the network from the input data. These maps capture various levels of detail, such as edges, textures, and more complex patterns as the data progresses through the layers. By visualizing these feature maps, we can better understand how the model identifies and processes important features in the input, offering valuable insights into its learning and decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1ea6252e5afd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name = \"OriginalSizeCNN\"\n",
    "model = src.model.OriginalSizeCNN()\n",
    "model_path = f\"./models/{name}.pth\"\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True, map_location=torch.device(device)))\n",
    "model.device = device\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73e1ecc1",
   "metadata": {},
   "source": [
    "## 3. Plot Sample Image\n",
    "In our dataset, we are working with audio clips. To process these audio signals in a Convolutional Neural Network (CNN), we first convert them into spectrograms, which represent the frequency content of the audio over time. Spectrograms transform audio data into a visual format, making it suitable for CNNs to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51006ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageId = 244\n",
    "sample_image = images[imageId]\n",
    "plot_spectrogram(plt.imread(f\"{feature_dataset_directory}/{image_files[imageId]}\"), image_files[imageId])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "285161b35dce1d87",
   "metadata": {},
   "source": [
    "## 4. Get feture map for first Convolutional Layer\n",
    "To extract feature maps from specific layers, we will create models that terminate at the desired layer. For instance, to obtain feature maps from the first convolutional layer (conv1), we construct a model that outputs the activations of that layer. Using these specialized models, we can feed an input (e.g., sample_image) and extract the corresponding feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf03f977daeeae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_layer = src.model.ModelWithLayerOutput(model,\"conv1\")\n",
    "c1_layer.device = device\n",
    "c1_feature_map = c1_layer(sample_image).detach().numpy()\n",
    "\n",
    "c1_feature_map.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9492b8dfff0b6b3f",
   "metadata": {},
   "source": [
    "## 5. Visualize feature map\n",
    "We see that the feature map generated from the output of the first convolutional layer of OriganSizeCNN conforms to the shape of (16, 128, 94). The first convolutional layer of OrginalCNN applies 16 3x3 filters resulting in 16 channels. We can visualize each of these activation maps to gain intuition for the types of things that the learned filters are detecting as signal at this layer. \n",
    "\n",
    "In the example below, we see that some channels are picking up on edges changes, while others focus on background and foreground shading/color. This is consistent with our expectation as this first layer should be honing in on very specific details in the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87d9712b4a0daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_map(feature_map, max_grid):\n",
    "    \n",
    "    fig, ax = plt.subplots(max_grid, max_grid, figsize=(7,7))\n",
    "    channel_idx = 0\n",
    "    \n",
    "    for i in range(max_grid):\n",
    "        for j in range(max_grid):\n",
    "            ax[i][j].imshow(feature_map[channel_idx,:,:])\n",
    "            ax[i][j].axis('off')\n",
    "            \n",
    "            channel_idx += 1\n",
    "            \n",
    "    fig.suptitle(f'Feature Map - Displaying {max_grid**2} of {feature_map.shape[0]} Channels')\n",
    "    fig.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_map(c1_feature_map, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3dec0ed3157bdea2",
   "metadata": {},
   "source": [
    "## 6. Feature maps from seconds layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4752328c46fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "c2_layer = src.model.ModelWithLayerOutput(model,\"conv2\")\n",
    "c2_layer.device = device\n",
    "c2_feature_map = c2_layer(sample_image).detach().numpy()\n",
    "c2_feature_map.shape\n",
    "plot_feature_map(c2_feature_map, 4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5149d065f433e641",
   "metadata": {},
   "source": [
    "## 7. Compare feature maps from third layer\n",
    "On the third convolutional layer of our model we will be extracting 64 channels. We expect them to focus on more semantically important features of our images, unfortunetly as we are working with spectrograms it is not intuitive for humans on what does the model focus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d2115c164b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "c3_layer = src.model.ModelWithLayerOutput(model,\"conv3\")\n",
    "c3_layer.device = device\n",
    "c3_feature_map = c3_layer(sample_image).detach().numpy()\n",
    "c3_feature_map.shape\n",
    "plot_feature_map(c3_feature_map, 8)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bdee942",
   "metadata": {},
   "source": [
    "# Similarity Search\n",
    "\n",
    "Now that we understand how to extract semantically meaningful features using a pretrained CNN, we can explore leveraging these features for scalable search. Facebook AI Similarity Search (FAISS) is a powerful and highly optimized library designed for this purpose. It enables efficient comparisons of a query record (e.g., a feature vector) against a stored database of feature vectors to retrieve the most similar ones.\n",
    "\n",
    "FAISS uses various metrics to calculate the \"similarity\" between vectors, with one of the simplest being proximity in Euclidean space—similar vectors are those that are closer together in this space. For more information on FAISS, feel free to explore this article.\n",
    "\n",
    "To demonstrate FAISS in action, we'll start by extracting feature maps from the first and third convolutional layers (c1_layer and c3_layer) for every image in our reduced dataset. Each feature map's vectors will then be added to its own FAISS index, enabling us to perform similarity searches efficiently. Finally, we’ll conduct similarity searches using these indices and visualize the comparative search results from the two feature maps for a given input image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c8610b9",
   "metadata": {},
   "source": [
    "## 8. Load feature maps for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f53ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "image_loader = prepare_dataset_loader(feature_dataset_directory, transform, batch_size)\n",
    "c3_layer.device = \"cpu\"\n",
    "c3_layer = c3_layer.to(\"cpu\")\n",
    "preds, feature_maps = process_predictions_and_features(c1_layer, image_loader)\n",
    "preds_c3, feature_maps_c3 = process_predictions_and_features(c3_layer, image_loader)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bce13ed",
   "metadata": {},
   "source": [
    "## 9. Create faiss indexes for feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02a83ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "feature_maps = np.vstack(feature_maps)\n",
    "feature_maps_c3 = np.vstack(feature_maps_c3)\n",
    "\n",
    "indicies = {}\n",
    "features = {'c1_layer': feature_maps,\n",
    "            'c3_layer': feature_maps_c3}\n",
    "\n",
    "for name, feature_map in features.items():\n",
    "    feature_dim = feature_map.shape[1]\n",
    "    index = faiss.IndexFlatL2(feature_dim)\n",
    "    index.add(feature_map)\n",
    "    \n",
    "    indicies[name] = index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b163b98",
   "metadata": {},
   "source": [
    "## 10. Define functions for getting n similar images from dataset using FAISS.\n",
    "This function compares a given query image against feature maps to find the most similar images. It then visualizes the top `k` most similar images for each convolutional layer's feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125bc0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "def wrap_title(title, width=25):\n",
    "    return \"\\n\".join(textwrap.wrap(title, width=width))\n",
    "    \n",
    "def get_similar(index, query_vec, k):\n",
    "    distances, indices = index.search(query_vec, k)\n",
    "    return distances, indices\n",
    "\n",
    "# Define function to visually compare top similar results for a given query image against feature maps\n",
    "def plot_n_similar(faiss_indicies, image_holder, image_files, feature_maps, query_idx, k):\n",
    "    \n",
    "    # Get similar vectors for each conv layer\n",
    "    sim_indicies = {}\n",
    "    for layer in faiss_indicies.keys():\n",
    "        dist, indic = get_similar(faiss_indicies[layer],\n",
    "                                  feature_maps[layer][query_idx:query_idx+1], k)\n",
    "        \n",
    "        sim_indicies[layer] = indic\n",
    "    \n",
    "    # Plot query image\n",
    "    plt.title(f'Query Image - {image_files[query_idx]}' )\n",
    "    query_img = image_holder[query_idx]\n",
    "    plt.imshow(query_img.squeeze(0), cmap=\"gray\")\n",
    "\n",
    "    \n",
    "    # Plot k most similar images for each layers feature map\n",
    "    fig, ax = plt.subplots(2, k)\n",
    "    \n",
    "    i = 0\n",
    "    for layer, indic in sim_indicies.items():\n",
    "        indic = indic.ravel()\n",
    "        for j in range(len(indic)):\n",
    "            ax[i][j].axis('off')\n",
    "            ax[i][j].imshow(image_holder[indic[j]].squeeze(0),cmap=\"gray\")\n",
    "            ax[i][j].set_title(wrap_title(f'Rank {j+1} - {layer} - {image_files[indic[j]]}'), fontsize=5)\n",
    "        i += 1\n",
    "        \n",
    "    fig.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    fig.suptitle(f'Top {k} Most Similar Images by Feature Map')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3819eedd",
   "metadata": {},
   "source": [
    "## 11. Plot sample results\n",
    "Using the similarity plotting function, we can easily compare the efficacy of each convolutional layer's feature map in extracting semantic meaning from an image. In the example below (query_idx = 200), we see that the first layer properly identifies person but returns wrong image. On the other hand the third layer returned exactly the image we querried as the fourth most similar result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968e962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_similar(faiss_indicies=indicies,\n",
    "               image_holder=images,\n",
    "               image_files=image_files,\n",
    "               feature_maps=features,\n",
    "               query_idx=200,\n",
    "               k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Feb  3 2024, 15:58:27) \n[Clang 15.0.0 (clang-1500.3.9.4)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "cccc1e4ed51fc8b9fd510a52ec83f3ba6504ae6c1f28f1731113cc11ad46be9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
